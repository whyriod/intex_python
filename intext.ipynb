{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's alive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "import pandas as pd\n",
    "from numpy import mean, std\n",
    "print(\"It's alive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, drop=[]):\n",
    "  import pandas as pd\n",
    "  df = pd.read_json(url)\n",
    "  if len(drop) > 0:\n",
    "    for col in drop:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "def bin_groups(df, percent=.05):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      for group, count in df[col].value_counts().iteritems():\n",
    "        if count / len(df) < percent:\n",
    "          df.loc[df[col] == group, col] = 'Other'\n",
    "  return df\n",
    "\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if df[col].isna().sum() / len(df) > cutoff:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "  return df\n",
    "\n",
    "def impute_mean(df):\n",
    "  from sklearn.impute import SimpleImputer\n",
    "  import pandas as pd, numpy as np\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "  return df\n",
    "\n",
    "def impute_KNN(df):\n",
    "  from sklearn.impute import KNNImputer\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "  df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
    "  imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "  return df\n",
    "      \n",
    "def impute_reg(df):\n",
    "  from sklearn.experimental import enable_iterative_imputer\n",
    "  from sklearn.impute import IterativeImputer\n",
    "  import pandas as pd\n",
    "  for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "  imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "  return df\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "  from sklearn.feature_selection import VarianceThreshold\n",
    "  import pandas as pd\n",
    "\n",
    "  if label != \"\":\n",
    "    X = df.drop(columns=[label])\n",
    "    \n",
    "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "  sel.fit_transform(X)\n",
    "\n",
    "  # Add the label back in after removing poor features\n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean, std\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "  print(f'Average R-squared:\\t{mean(scores)}')\n",
    "  return LinearRegression().fit(X, y)\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "  import pickle\n",
    "  pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "  import pickle\n",
    "  model = pickle.load(open(file_name, \"rb\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_id</th>\n",
       "      <th>crash_datetime</th>\n",
       "      <th>route</th>\n",
       "      <th>milepoint</th>\n",
       "      <th>lat_utm_y</th>\n",
       "      <th>long_utm_x</th>\n",
       "      <th>main_road_name</th>\n",
       "      <th>city</th>\n",
       "      <th>county_name</th>\n",
       "      <th>crash_severity_id</th>\n",
       "      <th>...</th>\n",
       "      <th>domestic_animal_related</th>\n",
       "      <th>overturn_rollover</th>\n",
       "      <th>commercial_motor_veh_involved</th>\n",
       "      <th>teenage_driver_involved</th>\n",
       "      <th>older_driver_involved</th>\n",
       "      <th>night_dark_condition</th>\n",
       "      <th>single_vehicle</th>\n",
       "      <th>distracted_driving</th>\n",
       "      <th>drowsy_driving</th>\n",
       "      <th>roadway_departure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11281387</td>\n",
       "      <td>2019-02-08T10:56:00.000</td>\n",
       "      <td>2258.0</td>\n",
       "      <td>4.560</td>\n",
       "      <td>4513124.085</td>\n",
       "      <td>422620.0512</td>\n",
       "      <td>900 WEST</td>\n",
       "      <td>SALT LAKE CITY</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11282089</td>\n",
       "      <td>2019-12-24T12:48:00.000</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>2.702</td>\n",
       "      <td>4483700.743</td>\n",
       "      <td>427458.6694</td>\n",
       "      <td>1215 E HIGHLAND DR</td>\n",
       "      <td>DRAPER</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11282092</td>\n",
       "      <td>2019-12-28T20:42:00.000</td>\n",
       "      <td>2062.0</td>\n",
       "      <td>0.351</td>\n",
       "      <td>4486609.366</td>\n",
       "      <td>426779.0152</td>\n",
       "      <td>1022 E DRAPER PKY</td>\n",
       "      <td>DRAPER</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11282098</td>\n",
       "      <td>2019-12-07T00:46:00.000</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>4484350.913</td>\n",
       "      <td>426397.6578</td>\n",
       "      <td>13463 S FORT ST</td>\n",
       "      <td>DRAPER</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11282101</td>\n",
       "      <td>2019-12-18T17:29:00.000</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>4485217.604</td>\n",
       "      <td>427714.0392</td>\n",
       "      <td>13043 S 1300 E</td>\n",
       "      <td>DRAPER</td>\n",
       "      <td>SALT LAKE</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   crash_id           crash_datetime   route  milepoint    lat_utm_y  \\\n",
       "0  11281387  2019-02-08T10:56:00.000  2258.0      4.560  4513124.085   \n",
       "1  11282089  2019-12-24T12:48:00.000  2082.0      2.702  4483700.743   \n",
       "2  11282092  2019-12-28T20:42:00.000  2062.0      0.351  4486609.366   \n",
       "3  11282098  2019-12-07T00:46:00.000  2048.0      0.438  4484350.913   \n",
       "4  11282101  2019-12-18T17:29:00.000  2068.0      0.796  4485217.604   \n",
       "\n",
       "    long_utm_x      main_road_name            city county_name  \\\n",
       "0  422620.0512            900 WEST  SALT LAKE CITY   SALT LAKE   \n",
       "1  427458.6694  1215 E HIGHLAND DR          DRAPER   SALT LAKE   \n",
       "2  426779.0152   1022 E DRAPER PKY          DRAPER   SALT LAKE   \n",
       "3  426397.6578     13463 S FORT ST          DRAPER   SALT LAKE   \n",
       "4  427714.0392      13043 S 1300 E          DRAPER   SALT LAKE   \n",
       "\n",
       "   crash_severity_id  ...  domestic_animal_related  overturn_rollover  \\\n",
       "0                  2  ...                    False              False   \n",
       "1                  1  ...                    False              False   \n",
       "2                  1  ...                    False              False   \n",
       "3                  3  ...                    False               True   \n",
       "4                  1  ...                    False              False   \n",
       "\n",
       "   commercial_motor_veh_involved  teenage_driver_involved  \\\n",
       "0                           True                    False   \n",
       "1                          False                     True   \n",
       "2                          False                     True   \n",
       "3                          False                    False   \n",
       "4                          False                     True   \n",
       "\n",
       "   older_driver_involved  night_dark_condition  single_vehicle  \\\n",
       "0                  False                 False           False   \n",
       "1                  False                 False           False   \n",
       "2                  False                  True           False   \n",
       "3                  False                  True            True   \n",
       "4                  False                  True           False   \n",
       "\n",
       "   distracted_driving  drowsy_driving  roadway_departure  \n",
       "0               False           False               True  \n",
       "1               False           False              False  \n",
       "2               False           False              False  \n",
       "3               False           False              False  \n",
       "4               False           False              False  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data('https://opendata.utah.gov/resource/herb-zqda.json', [])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other    1000\n",
      "Name: crash_datetime, dtype: int64\n",
      "\n",
      "Other    891\n",
      "I-15     109\n",
      "Name: main_road_name, dtype: int64\n",
      "\n",
      "Other                  383\n",
      "OUTSIDE CITY LIMITS    139\n",
      "DRAPER                  79\n",
      "SANDY                   66\n",
      "SALT LAKE CITY          65\n",
      "WEST VALLEY CITY        61\n",
      "WEST JORDAN             36\n",
      "PROVO                   35\n",
      "MURRAY                  33\n",
      "ST. GEORGE              30\n",
      "LAYTON                  29\n",
      "LEHI                    23\n",
      "SOUTH JORDAN            21\n",
      "Name: city, dtype: int64\n",
      "\n",
      "SALT LAKE     474\n",
      "UTAH          140\n",
      "Other         120\n",
      "DAVIS          95\n",
      "WEBER          59\n",
      "WASHINGTON     44\n",
      "BOX ELDER      36\n",
      "CACHE          32\n",
      "Name: county_name, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to update the DataFrame\n",
    "df = bin_groups(df, 0.02)\n",
    "\n",
    "# Check the value_counts() to see if it worked\n",
    "for col in df:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        print(df[col].value_counts())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature\t Percent Missing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature\\t\", \"Percent Missing\\n\")\n",
    "\n",
    "for col in df:\n",
    "    if df[col].isnull().sum() / len(df) > .5:\n",
    "        print(f'{col}\\t{round(df[col].isnull().sum() / len(df), 4) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash_id\n",
      "route\n",
      "milepoint\n",
      "lat_utm_y\n",
      "long_utm_x\n",
      "crash_severity_id\n",
      "work_zone_related\n",
      "pedestrian_involved\n",
      "bicyclist_involved\n",
      "motorcycle_involved\n",
      "improper_restraint\n",
      "unrestrained\n",
      "dui\n",
      "intersection_related\n",
      "wild_animal_related\n",
      "domestic_animal_related\n",
      "overturn_rollover\n",
      "commercial_motor_veh_involved\n",
      "teenage_driver_involved\n",
      "older_driver_involved\n",
      "night_dark_condition\n",
      "single_vehicle\n",
      "distracted_driving\n",
      "drowsy_driving\n",
      "roadway_departure\n",
      "crash_datetime_Other\n",
      "main_road_name_I-15\n",
      "main_road_name_Other\n",
      "city_DRAPER\n",
      "city_LAYTON\n",
      "city_LEHI\n",
      "city_MURRAY\n",
      "city_OUTSIDE CITY LIMITS\n",
      "city_Other\n",
      "city_PROVO\n",
      "city_SALT LAKE CITY\n",
      "city_SANDY\n",
      "city_SOUTH JORDAN\n",
      "city_ST. GEORGE\n",
      "city_WEST JORDAN\n",
      "city_WEST VALLEY CITY\n",
      "county_name_BOX ELDER\n",
      "county_name_CACHE\n",
      "county_name_DAVIS\n",
      "county_name_Other\n",
      "county_name_SALT LAKE\n",
      "county_name_UTAH\n",
      "county_name_WASHINGTON\n",
      "county_name_WEBER\n"
     ]
    }
   ],
   "source": [
    "# model = fit_crossvalidate_mlr(df, 5, 'crash_severity_id', True)\n",
    "df.dtypes\n",
    "\n",
    "#Dummy Codes for all remaining categorical values\n",
    "for col in df.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "         df = pd.get_dummies(df, columns=[col], prefix=col)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared scores: \n",
      "[ 0.17959354 -0.03383149  0.12853768  0.2226833   0.29213778  0.1108237\n",
      " -0.00116305  0.10259803  0.06475041  0.06229597  0.16958027  0.20671238\n",
      "  0.1761322   0.1216755  -0.00228994  0.05822365  0.10617973  0.05570843\n",
      "  0.19477251  0.075939    0.36007282  0.16280621  0.20744408  0.06263825\n",
      "  0.04887081  0.09282603  0.07454248  0.18635593  0.02006952  0.04727372\n",
      "  0.10430834  0.17391622  0.11750247  0.15356797  0.13937312]\n",
      "\n",
      "Average R-squared:\t0.12121793100139318\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "model = fit_crossvalidate_mlr(df, 7, 'crash_severity_id', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_crossvalidate_mlr(df, 7, 'crash_severity_id', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\whyri\\onedrive\\desktop\\one_folder\\stuff\\new455\\ml-venv2\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\whyri\\onedrive\\desktop\\one_folder\\stuff\\new455\\ml-venv2\\lib\\site-packages (from xgboost) (1.22.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: scipy in c:\\users\\whyri\\onedrive\\desktop\\one_folder\\stuff\\new455\\ml-venv2\\lib\\site-packages (from xgboost) (1.8.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1326, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 692, in fit\n",
      "    self._update_feature_log_prob(alpha)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1452, in _update_feature_log_prob\n",
      "    np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 432. MiB for an array with shape (5, 11315547) and data type float64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1326, in fit\n",
      "    return super().fit(X, y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 692, in fit\n",
      "    self._update_feature_log_prob(alpha)\n",
      "  File \"c:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1452, in _update_feature_log_prob\n",
      "    np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 432. MiB for an array with shape (5, 11314242) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Accuracy\n",
      "Ridge                0.7140\n",
      "SVM                  0.7140\n",
      "Logistic             0.7130\n",
      "Bagging              0.7115\n",
      "Voting               0.6790\n",
      "KNN                  0.6690\n",
      "PassiveAggressive    0.6645\n",
      "RandomForest         0.6505\n",
      "HistGradient         0.6500\n",
      "XGBoost              0.6455\n",
      "ExtraTrees           0.6355\n",
      "GradBoost            0.6235\n",
      "SGD                  0.5340\n",
      "AdaBoost             0.4245\n",
      "NeuralN              0.1740\n",
      "Perceptron           0.1150\n",
      "NaiveBayes              NaN\n"
     ]
    }
   ],
   "source": [
    "# %pip install xgboost\n",
    "def fit_crossvalidate_clf(df, label, k=10, r=5, repeat=True):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "  from sklearn import svm\n",
    "  from sklearn.naive_bayes import CategoricalNB\n",
    "  from xgboost import XGBClassifier\n",
    "  from sklearn import preprocessing\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  \n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "  \n",
    "  # Create the model objects\n",
    "  model_log = lm.LogisticRegression(max_iter=100)\n",
    "  model_logcv = lm.RidgeClassifier()\n",
    "  model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "  model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=12345, tol=1e-3)\n",
    "  model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "  model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "  model_nb = CategoricalNB()\n",
    "  model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "  model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "  model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=12345)\n",
    "  model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "  model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "  model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "  model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=12345)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "  final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=12345)\n",
    "  model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "  model_xgb = XGBClassifier()\n",
    "  model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=12345)\n",
    "\n",
    "  # Fit a cross-validated R squared score and add it to the dict\n",
    "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dictionary; make sure the keys have the same names as the list above\n",
    "  models['Logistic'] = model_log\n",
    "  models['Ridge'] = model_logcv\n",
    "  models['SGD'] = model_sgd\n",
    "  models['PassiveAggressive'] = model_pa\n",
    "  models['Perceptron'] = model_per\n",
    "  models['KNN'] = model_knn\n",
    "  models['SVM'] = model_svm\n",
    "  models['NaiveBayes'] = model_nb\n",
    "  models['Bagging'] = model_bag\n",
    "  models['AdaBoost'] = model_ada\n",
    "  models['ExtraTrees'] = model_ext\n",
    "  models['RandomForest'] = model_rf\n",
    "  models['HistGradient'] = model_hgb\n",
    "  models['Voting'] = model_vot\n",
    "  models['GradBoost'] = model_gb\n",
    "  models['XGBoost'] = model_xgb\n",
    "  models['NeuralN'] = model_nn\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "  df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)\n",
    "\n",
    "# Data cleaning and preparation pipeline\n",
    "df_new = get_data('https://opendata.utah.gov/resource/herb-zqda.json', [])\n",
    "df_new.head()\n",
    "df_new = bin_groups(df_new)\n",
    "df_new = drop_columns_missing_data(df_new)\n",
    "\n",
    "# Drop the label so it does not get dummy-coded, then join it back in after\n",
    "df_new = impute_mean(df_new.drop(columns=[\"crash_severity_id\"])).join(df_new.crash_severity_id)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df_new = fs_variance(df_new, label=\"crash_severity_id\", p=.5)\n",
    "model = fit_crossvalidate_clf(df_new, \"crash_severity_id\", 5, 2)\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'best_clf_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (5, 5), indices imply (5, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\intext.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=4'>5</a>\u001b[0m df_new\u001b[39m.\u001b[39mdrop_duplicates(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=5'>6</a>\u001b[0m coefs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39;49mcoef_,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=7'>8</a>\u001b[0m     columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mCoefficients\u001b[39;49m\u001b[39m'\u001b[39;49m], index\u001b[39m=\u001b[39;49mdf_new\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mcrash_severity_id\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mcolumns\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=10'>11</a>\u001b[0m coefs\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mCoefficients\u001b[39m\u001b[39m'\u001b[39m], ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/intext.ipynb#ch0000013?line=12'>13</a>\u001b[0m coefs\u001b[39m.\u001b[39mplot(kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbarh\u001b[39m\u001b[39m'\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m30\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=683'>684</a>\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=684'>685</a>\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=685'>686</a>\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=690'>691</a>\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=691'>692</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=692'>693</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=693'>694</a>\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=694'>695</a>\u001b[0m             data,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=695'>696</a>\u001b[0m             index,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=696'>697</a>\u001b[0m             columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=697'>698</a>\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=698'>699</a>\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=699'>700</a>\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=700'>701</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=702'>703</a>\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/frame.py?line=703'>704</a>\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=345'>346</a>\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=346'>347</a>\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=347'>348</a>\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=348'>349</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=350'>351</a>\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=352'>353</a>\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=354'>355</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\whyri\\OneDrive\\Desktop\\One_Folder\\stuff\\New455\\ml-venv2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=419'>420</a>\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=420'>421</a>\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> <a href='file:///c%3A/Users/whyri/OneDrive/Desktop/One_Folder/stuff/New455/ml-venv2/lib/site-packages/pandas/core/internals/construction.py?line=421'>422</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (5, 5), indices imply (5, 1)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_new.drop_duplicates(inplace=True)\n",
    "coefs = pd.DataFrame(\n",
    "    model.coef_,\n",
    "    columns=['Coefficients'], index=df_new.drop(columns=['crash_severity_id']).columns\n",
    ")\n",
    "\n",
    "coefs.sort_values(by=['Coefficients'], ascending=False, inplace=True)\n",
    "\n",
    "coefs.plot(kind='barh', figsize=(10, 30))\n",
    "plt.title('MLR model')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3bee42fb0e871b9539e3a7d65fb99a7538b7a6c6b0f746d91eb4ff047ffb1ab"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('ml-venv2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
